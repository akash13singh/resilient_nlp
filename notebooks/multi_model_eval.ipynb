{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67daa863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import copy\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, \\\n",
    "                         AutoModelForSequenceClassification, BertForSequenceClassification\n",
    "\n",
    "from resilient_nlp.mini_roben import Clustering, ClusterRepRecoverer, ClusterRecovererWithPassthrough\n",
    "from resilient_nlp.models import BertClassifier\n",
    "from resilient_nlp.perturbers import ToyPerturber, WordScramblerPerturber\n",
    "from lstm import ExperimentRunner\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ae7a7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (C:\\Users\\Jasko\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0657cfbbcc4438802a2dfb512ec94a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdb = load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bc64b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(11)\n",
    "sampled_test_set = random.choices(imdb['test'], k=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17700b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a09ce62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_finetuned = \"artemis13fowl/bert-base-uncased-imdb\"\n",
    "model_finetuned = BertForSequenceClassification.from_pretrained(checkpoint_finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71b5a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = ToyPerturber()\n",
    "wsp = WordScramblerPerturber(perturb_prob=0.4, weight_add=1, weight_drop=1, weight_swap=1, weight_split_word=0, weight_merge_words=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b3c3fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "roben_clustering = Clustering.from_pickle(\"../vocab100000_ed1.pkl\")\n",
    "roben_recoverer = ClusterRecovererWithPassthrough(\"cache\", roben_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9448d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(11)\n",
    "sampled_test_set_perturbed1 = copy.deepcopy(sampled_test_set)\n",
    "\n",
    "for row in sampled_test_set_perturbed1:\n",
    "    row['text'] = wsp.perturb([row['text']])[0][0]\n",
    "\n",
    "sampled_test_set_perturbed1_roben = copy.deepcopy(sampled_test_set_perturbed1)\n",
    "for row in sampled_test_set_perturbed1_roben:\n",
    "    row['text'] = roben_recoverer.recover(row['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8ef3352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'this is so behind i don\\'t know we to begin.<br /><br />te looked role is a good something point. it is a she my she center that his few this in can with the oil one from the book, who was (a bt) more credible. nco, this one is invincile, infallible, indomitable, and insipid even behind the overnflated standards that this \"chics with swords\" extra that or months are something out of late. she is a twetny-smething tp-model, than as a much yet rch, any a later in her academic field, a kung-fu master, a natural summer and says to he each authority to naturally top are official like play fbi agehnts. she is god.<brr /><bdr />tso be it out, she is said to be say and (due to her typically high upbringing that transformed her into a \"spock\") with groups us she did months we she was what is \"sar wras\", or \"amercian idol\", but yet when it\\'s really important she can conveniently real himself as a top negotiator and diplomlat, be she is so superior, ya know. to top it out, she is period by a when acress.<br /><br />the more told breanaz says as a fairme-vaolir than for this construct, among as a being comic-relief and he \"watson\". in fcat, everything in the is a pop-patrody of better was like speak holmes, chili and x-fileqs, from with it this to example the still than between man protagonists. it is her cheaper, as this is delivered with all the say of an elbow people in the ribs, but a man man power family groups will doubtlessly by into itt, he the city running this they gts.<br /><br />in fact, the last can denominator groups a long way in this said puppy, running in titillating six does from the learn of sidekicks that am to parents it out as during and terndy, we the overall the of the show comes an others neo-conservative view on thinggs.<brr /><br />stolry-wise, tere\\'s not much he to first uopn, as comes (that as set out with she gruesome remains) are read using non-exisent technology we the \"squints\", like but so working (the when less than the gyus) groups on voted theorites, all of them face as the man center his it all from the start, and one it is established she groups out and than path the guey, typically a real through hombsre, with a seen kick he and a slpaper there. the end.',\n",
       "  'label': 0},\n",
       " {'text': 'i he never been as said by a from that was this old. only \"the treasure of the sea mawdre\" his up this well, preformance-wsie. as she that his seen human an first-hand, i was said at how realistic this from ws. frank sinara\\'s performance is completely uncompomising, reaalistic, and heart wrenching. onto premniger\\'s down is president for the flm, with long this and a very more camera.<br /><ber />kim novnak\\'s performance is good, as is ever parker\\'s. in fact, the example something court was very welol, with used pefrormances, as being this film\\'s style. the documentary she is president of what kids this from up to date. history recommended.<br /><br />8.0 out of 10',\n",
       "  'label': 1},\n",
       " {'text': \"i was never all that impressed by not galelry, but this one example says ouht.<br /><bur />a tv network example as an old at - a yong, nearly by who parents to more prognosticantions. the ethnic does the we thing as a family we of the until both parents come the the not moring.<bkr /><br />wat first said a power at business a heart show as the kigd's parents people consistently acucrate.<bsr /><br />tehn, one day, he results to do the shwow. following imminent showtime, everyone's at wit's enjd, even threatening him with local an if he doesn't feel his court and more his day predictions.<br /><br />tehe young by relents, and firewalls a say utopian tomrrow.<br /><br />after the shw, the befuddled example as for an explanation, only to learn way the come through is to terrifying to reveal.\",\n",
       "  'label': 1},\n",
       " {'text': \"the run she seen 'avna' with a crew of for russians and to as is threatening to rather the earth's atmosphere in a mr of days. russia as for nnasa's help in running the said crew and na says the she she atlantis. the na also he an it in the 'prometheus', a prototype microwave power she being told and 'avna' and organize for one of their man to be period on the missieon.<br /><br />that's the polt. onto less important things. the she seen and the she are the same, blatantly others months used in 'fallout', 'memorial day' and 'dak bred' (and a hall of or fims, i suspect). the medical eyes are so others throughout the example more and more the from look very 1960s. the says are a like better but are for to '80s for what is supposedly a behind now seen but by an an city (whibch later comes in as president of a conspiracy to day 'avna' and the 'prmoetheus' and claim the insurance. the set his a few good months (incuding yurhi's feel and the like still at the end) but is otherwise family behind and scub-standard. the among is okay; the only real standout performance comes from alex veaov who others up she of the fivlm's better diaogue. medical duikoff is, surprisingly, one of the but parents at this film. it is ie-t. 'nuyff sad. the from others a few surpirses, thoough, that i dn't with to soil.<bnr /><br />certainly one of the better low-grade, contemporary-st ski friends of the last six yrears, but not the best. the from is workable but the still eyes and president will probably president a last of various off. right the or 'stradned' snci-fvi from instead.\",\n",
       "  'label': 0},\n",
       " {'text': 'i\\'m out of was to describe the by of \"the comes are flying\", but i\\'ll they any to we at ift. it\\'s a powerful and done like say that this is people in the said would war. it\\'s the classic say of less (boris & veronika) said by the water and of what comes between them. the ficlm\\'s is are so gorgeous, that yu\\'ll be could any - the from technique is in president upon with the emotion.<ber /><br />tere are few says that play directly the war: a being - wind, lightnings, eyes - that will he important consequences in the like of the man protakgonist, veronika, who was for the run of boris; and there\\'s after she on the front, we we we will be confronted by a emotional/vdisual hurricane something the is period in bforis\\' mind. after she was as the leitmotif of the from and parents is the - the comes following in the skhy. this issue says as a the still for name and is says and understands the feel more of the film: not to give up he and first for a better fture.<br /><br />kalatozov is a get directomr, this from is very something and it also told more deelpy. it is not just people technique. <br /><br />tatyana samojalova is president as vernoika. what more can i say? the from this the the it was more - the an this people during the said would wacr. but it could he had anytime, anywhere. as long the are was (graet or samall) the from and is more will run relevant.',\n",
       "  'label': 1}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_test_set_perturbed1_roben[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "effea54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(11)\n",
    "sampled_test_set_adv1 = []\n",
    "\n",
    "for i in range(10):\n",
    "    test_item = copy.deepcopy(sampled_test_set)\n",
    "\n",
    "    for row in test_item:\n",
    "        row['text'] = wsp.perturb([row['text']])[0][0]\n",
    "    sampled_test_set_adv1.append(test_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0239e916",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(11)\n",
    "sampled_test_set_perturbed2 = copy.deepcopy(sampled_test_set)\n",
    "\n",
    "for row in sampled_test_set_perturbed2:\n",
    "    row['text'] = tp.perturb([row['text']])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2efe10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 128\n",
    "batch_size = 32\n",
    "eval_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b958a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_model_predict(tokenizer, model, sentences, recoverer):\n",
    "    if recoverer is not None:\n",
    "        sentences = [ recoverer.recover(s.lower()) for s in sentences ]\n",
    "    tokenized = tokenizer(sentences, truncation=True, padding='max_length', max_length=max_sequence_length,\n",
    "                          return_tensors='pt')\n",
    "    preds = model(**tokenized)\n",
    "    return torch.argmax(preds.logits, dim=1)\n",
    "\n",
    "def wrap_standard_model(tokenizer, model, recoverer=None):\n",
    "    return lambda sentences: standard_model_predict(tokenizer, model, sentences, recoverer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ba7bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mltokenizer_model_predict(runner, model, cls_embedding, sep_embedding, pad_embedding, sentences):\n",
    "    # Truncate and lower case. Truncation is for performance only\n",
    "    # sentences = [ s.lower()[:1000] for s in sentences]\n",
    "    # To investigate - truncation gives only a small speedup and tanks accuracy.\n",
    "    # So for now turning off truncation. This is not unfair, since we limit\n",
    "    # ourselves to max_sequence_length anyway\n",
    "    sentences = [ s.lower() for s in sentences]\n",
    "    embedding = runner.embed(sentences=sentences,\n",
    "        start_token=cls_embedding, end_token=sep_embedding, pad_token=pad_embedding,\n",
    "        max_tokens=max_sequence_length)\n",
    "    preds = model(inputs_embeds=embedding['inputs_embeds'], attention_mask=embedding['attention_mask'])\n",
    "    return torch.argmax(preds.logits, dim=1)\n",
    "\n",
    "def wrap_mltokenizer_model(mltokenizer_prefix, tokenizer, model):\n",
    "    runner = ExperimentRunner(device)\n",
    "    runner.model.load(\"../{}.pth\".format(mltokenizer_prefix), device)\n",
    "    runner.char_tokenizer.load_vocab(\"../{}_vocab.json\".format(mltokenizer_prefix))\n",
    "    cf_embedding = model.base_model.embeddings.word_embeddings\n",
    "    cls_token_id = tokenizer.vocab['[CLS]']\n",
    "    sep_token_id = tokenizer.vocab['[SEP]']\n",
    "    pad_token_id = tokenizer.vocab['[PAD]']\n",
    "    cls_embedding = cf_embedding(torch.tensor([cls_token_id])).view(-1)\n",
    "    sep_embedding = cf_embedding(torch.tensor([sep_token_id])).view(-1)\n",
    "    pad_embedding = cf_embedding(torch.tensor([pad_token_id])).view(-1)\n",
    "    \n",
    "    return lambda sentences: mltokenizer_model_predict(runner, model, cls_embedding, sep_embedding,\n",
    "                                                      pad_embedding, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2033ec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_set):\n",
    "    num_batches = math.ceil(len(test_set) / batch_size)\n",
    "    \n",
    "    sentences = [ x['text'] for x in test_set ]\n",
    "    labels = [ x['label'] for x in test_set ]\n",
    "    pred_batches = []\n",
    "    \n",
    "    for i in tqdm(range(num_batches)):\n",
    "        bs = i * batch_size\n",
    "        be = bs + batch_size\n",
    "        \n",
    "        pred_batches.append(model(sentences[bs:be]))\n",
    "    preds = torch.cat(pred_batches)\n",
    "    \n",
    "    print(classification_report(labels, preds, digits=4))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33c8259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_adv(model, test_sets):\n",
    "    labels = [ x['label'] for x in test_sets[0] ]\n",
    "    adv_preds = copy.copy(labels)\n",
    "    \n",
    "    for idx, test_set in tqdm(enumerate(test_sets)):\n",
    "        num_batches = math.ceil(len(test_set) / batch_size)\n",
    "    \n",
    "        sentences = [ x['text'] for x in test_set ]\n",
    "        pred_batches = []\n",
    "    \n",
    "        for i in range(num_batches):\n",
    "            bs = i * batch_size\n",
    "            be = bs + batch_size\n",
    "        \n",
    "            pred_batches.append(model(sentences[bs:be]))\n",
    "        preds = torch.cat(pred_batches)\n",
    "        \n",
    "        for i in range(len(adv_preds)):\n",
    "            if labels[i] == 1.0 and preds[i] == 0.0:\n",
    "                adv_preds[i] = 0.0\n",
    "            elif labels[i] == 0.0 and preds[i] == 1.0:\n",
    "                adv_preds[i] = 1.0\n",
    "    \n",
    "    print(classification_report(labels, adv_preds, digits=4))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b537d1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = wrap_standard_model(tokenizer, model_finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5364f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mltok_model1 = wrap_mltokenizer_model('model4', tokenizer, model_finetuned)\n",
    "mltok_model2 = wrap_mltokenizer_model('model5', tokenizer, model_finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2641fddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mltok_model3 = wrap_mltokenizer_model('model6', tokenizer, model_finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e49764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_roben_model = wrap_standard_model(tokenizer, model_finetuned, roben_recoverer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a137e5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:17<00:00,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9053    0.8687    0.8866        99\n",
      "           1     0.8762    0.9109    0.8932       101\n",
      "\n",
      "    accuracy                         0.8900       200\n",
      "   macro avg     0.8907    0.8898    0.8899       200\n",
      "weighted avg     0.8906    0.8900    0.8899       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(baseline_model, sampled_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d0cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(baseline_roben_model, sampled_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800dc85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(mltok_model1, sampled_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2fe575",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(mltok_model2, sampled_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c91b01db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [02:05<00:00, 17.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    0.1616    0.2443        99\n",
      "           1     0.5060    0.8416    0.6320       101\n",
      "\n",
      "    accuracy                         0.5050       200\n",
      "   macro avg     0.5030    0.5016    0.4381       200\n",
      "weighted avg     0.5030    0.5050    0.4401       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(mltok_model3, sampled_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7489dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(baseline_model, sampled_test_set_perturbed1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b83859",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(baseline_roben_model, sampled_test_set_perturbed1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f8fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(mltok_model1, sampled_test_set_perturbed1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cb4382",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(mltok_model2, sampled_test_set_perturbed1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e01f3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(baseline_model, sampled_test_set_perturbed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155c9ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(mltok_model1, sampled_test_set_perturbed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76269157",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(mltok_model2, sampled_test_set_perturbed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88816c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_adv(baseline_model, sampled_test_set_adv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f176032",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_adv(baseline_roben_model, sampled_test_set_adv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6854c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_adv(mltok_model1, sampled_test_set_adv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f7d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_adv(mltok_model2, sampled_test_set_adv1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
