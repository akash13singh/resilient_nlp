{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67daa863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import copy\n",
    "import cProfile\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "import random\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, \\\n",
    "                         AutoModelForSequenceClassification, BertForSequenceClassification\n",
    "\n",
    "from resilient_nlp.mini_roben import Clustering, ClusterRepRecoverer, ClusterRecovererWithPassthrough\n",
    "from resilient_nlp.models import BertClassifier\n",
    "from resilient_nlp.perturbers import ToyPerturber, WordScramblerPerturber\n",
    "from runner import ExperimentRunner\n",
    "from word_score_attack import BertWordScoreAttack\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae7a7c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imdb = load_dataset('imdb')\n",
    "imdb = load_dataset('artemis13fowl/imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc64b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(11)\n",
    "sampled_test_set = imdb['attack_eval_truncated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17700b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09ce62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_finetuned = \"artemis13fowl/bert-base-uncased-imdb\"\n",
    "model_finetuned = BertForSequenceClassification.from_pretrained(checkpoint_finetuned).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b5a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsp = WordScramblerPerturber(perturb_prob=0.4, weight_add=1, weight_drop=1, weight_swap=1, weight_split_word=1, weight_merge_words=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3c3fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "roben_clustering = Clustering.from_pickle(\"../vocab100000_ed1.pkl\")\n",
    "roben_recoverer = ClusterRecovererWithPassthrough(\"cache\", roben_clustering)\n",
    "roben_clustering2 = Clustering.from_pickle(\"../vocab100000_ed1_gamma0.3.pkl\")\n",
    "roben_recoverer2 = ClusterRecovererWithPassthrough(\"cache\", roben_clustering2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9448d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(11)\n",
    "sampled_test_set_perturbed = copy.deepcopy(sampled_test_set)\n",
    "\n",
    "for row in sampled_test_set_perturbed:\n",
    "    row['text'] = wsp.perturb([row['text']])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effea54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(11)\n",
    "sampled_test_set_adv = []\n",
    "\n",
    "for i in range(10):\n",
    "    test_item = copy.deepcopy(sampled_test_set)\n",
    "\n",
    "    for row in test_item:\n",
    "        row['text'] = wsp.perturb([row['text']])[0][0]\n",
    "    sampled_test_set_adv.append(test_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2efe10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 128\n",
    "batch_size = 32\n",
    "eval_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b958a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_model_predict(tokenizer, model, sentences, recoverer, return_pred_tensor):\n",
    "    if recoverer is not None:\n",
    "        sentences = [ recoverer.recover(s.lower()) for s in sentences ]\n",
    "    tokenized = tokenizer(sentences, truncation=True, padding='max_length', max_length=max_sequence_length,\n",
    "                          return_tensors='pt')\n",
    "    tokenized = { k: v.to(device) for k, v in tokenized.items() }\n",
    "    preds = model(**tokenized)\n",
    "    if return_pred_tensor:\n",
    "        return preds\n",
    "    else:\n",
    "        return torch.argmax(preds.logits, dim=1)\n",
    "\n",
    "def wrap_standard_model(tokenizer, model, recoverer=None, return_pred_tensor=True):\n",
    "    return lambda sentences: standard_model_predict(tokenizer, model, sentences, recoverer, return_pred_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba7bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mltokenizer_model_predict(runner, model, cls_embedding, sep_embedding, pad_embedding, sentences, return_pred_tensor):\n",
    "    # Truncate and lower case. Truncation is for performance only\n",
    "    sentences = [ s.lower()[:5*max_sequence_length] for s in sentences]\n",
    "    embedding = runner.embed(sentences=sentences,\n",
    "        start_token=cls_embedding, end_token=sep_embedding, pad_token=pad_embedding,\n",
    "        max_tokens=max_sequence_length)\n",
    "    preds = model(inputs_embeds=embedding['inputs_embeds'], attention_mask=embedding['attention_mask'])\n",
    "    if return_pred_tensor:\n",
    "        return preds\n",
    "    else:\n",
    "        return torch.argmax(preds.logits, dim=1)\n",
    "\n",
    "def wrap_mltokenizer_model(mltokenizer_prefix, tokenizer, model, return_pred_tensor=True):\n",
    "    filename = \"../{}.pth\".format(mltokenizer_prefix)\n",
    "    runner = ExperimentRunner(device, model_filename=filename)\n",
    "    cf_embedding = model.base_model.embeddings.word_embeddings\n",
    "    cls_token_id = tokenizer.vocab['[CLS]']\n",
    "    sep_token_id = tokenizer.vocab['[SEP]']\n",
    "    pad_token_id = tokenizer.vocab['[PAD]']\n",
    "    cls_embedding = cf_embedding(torch.tensor([cls_token_id], device=device)).view(-1)\n",
    "    sep_embedding = cf_embedding(torch.tensor([sep_token_id], device=device)).view(-1)\n",
    "    pad_embedding = cf_embedding(torch.tensor([pad_token_id], device=device)).view(-1)\n",
    "    \n",
    "    return lambda sentences: mltokenizer_model_predict(runner, model, cls_embedding, sep_embedding,\n",
    "                                                      pad_embedding, sentences, return_pred_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2033ec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model(model, test_set):\n",
    "    num_batches = math.ceil(len(test_set) / batch_size)\n",
    "    \n",
    "    sentences = [ x['text'] for x in test_set ]\n",
    "    labels = [ x['label'] for x in test_set ]\n",
    "    pred_batches = []\n",
    "    \n",
    "    for i in tqdm(range(num_batches)):\n",
    "        bs = i * batch_size\n",
    "        be = bs + batch_size\n",
    "        \n",
    "        output = model(sentences[bs:be])\n",
    "        \n",
    "        pred_batches.append(torch.argmax(output.logits, dim=1).detach().cpu())\n",
    "    preds = torch.cat(pred_batches)\n",
    "    \n",
    "    print(classification_report(labels, preds, digits=4))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c8259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model_adv(model, test_sets):\n",
    "    labels = [ x['label'] for x in test_sets[0] ]\n",
    "    adv_preds = copy.copy(labels)\n",
    "    accuracy_list = []\n",
    "    f1_list = []\n",
    "    \n",
    "    for idx in tqdm(range(len(test_sets))):\n",
    "        test_set = test_sets[idx]\n",
    "        num_batches = math.ceil(len(test_set) / batch_size)\n",
    "    \n",
    "        sentences = [ x['text'] for x in test_set ]\n",
    "        pred_batches = []\n",
    "    \n",
    "        for i in range(num_batches):\n",
    "            bs = i * batch_size\n",
    "            be = bs + batch_size\n",
    "        \n",
    "            output = model(sentences[bs:be])\n",
    "        \n",
    "            pred_batches.append(torch.argmax(output.logits, dim=1).detach().cpu())\n",
    "        preds = torch.cat(pred_batches)\n",
    "        \n",
    "        for i in range(len(adv_preds)):\n",
    "            if labels[i] == 1.0 and preds[i] == 0.0:\n",
    "                adv_preds[i] = 0.0\n",
    "            elif labels[i] == 0.0 and preds[i] == 1.0:\n",
    "                adv_preds[i] = 1.0\n",
    "\n",
    "        accuracy_list.append(accuracy_score(labels, adv_preds))\n",
    "        f1_list.append(f1_score(labels, adv_preds, average='macro'))\n",
    "    \n",
    "    print(classification_report(labels, adv_preds, digits=4))    \n",
    "    \n",
    "    return accuracy_list, f1_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26be9ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model_word_score(model, test_set):\n",
    "    attacker = BertWordScoreAttack(\n",
    "        WordScramblerPerturber(perturb_prob=1, weight_add=1, weight_drop=1, weight_swap=1, weight_split_word=1,\n",
    "                               weight_merge_words=1),\n",
    "        \"../output/imdb_word_scores.json\", model, tokenizer=None, max_sequence_length=max_sequence_length\n",
    "    )\n",
    "\n",
    "    res = attacker.attack(test_set, max_tokens_to_query=10, max_tries_per_token=2, mode=0, print_summary=False)\n",
    "    \n",
    "    print(classification_report(res['ground_truth'], res['perturbed_preds'], digits=4))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b537d1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = wrap_standard_model(tokenizer, model_finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5364f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mltok_model = wrap_mltokenizer_model('output/64k_lstm_all_pert_finetuned', tokenizer, model_finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e49764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_roben_model = wrap_standard_model(tokenizer, model_finetuned, roben_recoverer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb23d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_roben_model2 = wrap_standard_model(tokenizer, model_finetuned, roben_recoverer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a137e5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(baseline_model, sampled_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d0cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(baseline_roben_model, sampled_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b3e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(baseline_roben_model2, sampled_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800dc85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(mltok_model, sampled_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7489dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(baseline_model, sampled_test_set_perturbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b83859",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(baseline_roben_model, sampled_test_set_perturbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea55597",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(baseline_roben_model2, sampled_test_set_perturbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f8fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(mltok_model, sampled_test_set_perturbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88816c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_adv(baseline_model, sampled_test_set_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f176032",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_adv(baseline_roben_model, sampled_test_set_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6854c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_adv(mltok_model, sampled_test_set_adv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
