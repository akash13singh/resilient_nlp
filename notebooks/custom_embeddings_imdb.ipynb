{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faf07b01",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6afc56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from lstm import ExperimentRunner\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaed86d5",
   "metadata": {},
   "source": [
    "Load the tokenizer and the finetuned model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "036c9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"artemis13fowl/bert-base-uncased-imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7f7fa4",
   "metadata": {},
   "source": [
    "Load the trained tokenizer. It has only been trained for 2 epochs on 64k sentences, so it's not mind-blowing.\n",
    "\n",
    "For now you need to manually get it and put it in the root of the resilient_nlp repo, sorry. You can get the model here: https://resilient-nlp.s3.us-west-2.amazonaws.com/tok_bert_base_uncased_64k_sentences_2_epochs.pth and the vocab here: https://resilient-nlp.s3.us-west-2.amazonaws.com/tok_bert_base_uncased_64k_sentences_2_epochs_vocab.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59323f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = ExperimentRunner(device, model_name=\"bert-base-uncased\")\n",
    "runner.model.load(\"../tok_bert_base_uncased_64k_sentences_2_epochs.pth\", device)\n",
    "runner.char_tokenizer.load_vocab(\"../tok_bert_base_uncased_64k_sentences_2_epochs_vocab.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e617511",
   "metadata": {},
   "source": [
    "Some simple test sentences for sanity checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "740e6fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "  \"I really enjoyed this movie\",\n",
    "  \"Worst two hours I have spent in my life\",\n",
    "  \"My worries and fears about this movie were swept away within the first fifteen minutes\",\n",
    "  \"My worries and fears about this movie were fully confirmed within the first fifteen minutes\",\n",
    "  \"My worries and fears about this movie were rendered meaningless within the first fifteen minutes\",\n",
    "  \"Movie was very good\",\n",
    "  \"It was enjoyable\",\n",
    "  \"Very boring unfortunately\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8915c3c",
   "metadata": {},
   "source": [
    "First, use the regular bert-base-uncased tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da93dcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.010627184063196182, 0.5175461769104004, -0.2631071209907532, 0.04238038882613182, 0.40291687846183777, 0.19839873909950256, -0.7841276526451111, 0.3104897141456604, 0.25195378065109253, 0.1873702108860016, -0.5451260209083557, 0.27750512957572937, 0.20487289130687714, -0.6549283266067505, -0.551720142364502, -0.5405393242835999]\n"
     ]
    }
   ],
   "source": [
    "tokenizer_result = tokenizer(sentences, return_tensors='pt', padding=True)\n",
    "\n",
    "model_result = model(\n",
    "    input_ids=tokenizer_result['input_ids'],\n",
    "#    attention_mask=tokenizer_result['attention_mask'],\n",
    "#    token_type_ids=tokenizer_result['token_type_ids'],\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "\n",
    "print(model_result['hidden_states'][0][0,1,:16].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5eb3c0",
   "metadata": {},
   "source": [
    "Next, manually embed using BERT's embedding before passing to the model. This is to verify that the model behaves identically (i.e. it turns out that positional embeddings, etc. are added automatically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57a3ee65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.010627184063196182, 0.5175461769104004, -0.2631071209907532, 0.04238038882613182, 0.40291687846183777, 0.19839873909950256, -0.7841276526451111, 0.3104897141456604, 0.25195378065109253, 0.1873702108860016, -0.5451260209083557, 0.27750512957572937, 0.20487289130687714, -0.6549283266067505, -0.551720142364502, -0.5405393242835999]\n"
     ]
    }
   ],
   "source": [
    "cf_embedding = model.base_model.embeddings.word_embeddings\n",
    "cf_embedding_result = cf_embedding(tokenizer_result['input_ids'])\n",
    "\n",
    "model_result_2 = model(\n",
    "    inputs_embeds=cf_embedding_result,\n",
    "#    attention_mask=tokenizer_result['attention_mask'],\n",
    "#    token_type_ids=tokenizer_result['token_type_ids'],\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "\n",
    "print(model_result_2['hidden_states'][0][0,1,:16].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348fcd73",
   "metadata": {},
   "source": [
    "Now the fun part :). Let's use our trained embeddings. First though let's get the representations of the [CLS] and [SEP] tokens (not predicted by the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a98317d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token_id = tokenizer.vocab['[CLS]']\n",
    "sep_token_id = tokenizer.vocab['[SEP]']\n",
    "cls_embedding = cf_embedding(torch.tensor([cls_token_id])).view(1, 1, 768)\n",
    "sep_embedding = cf_embedding(torch.tensor([sep_token_id])).view(1, 1, 768)\n",
    "\n",
    "rep_cls_embedding = cls_embedding.expand(len(sentences), 1, 768)\n",
    "rep_sep_embedding = sep_embedding.expand(len(sentences), 1, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fc9255",
   "metadata": {},
   "source": [
    "Now actually embed the input sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe0cc42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_embedding, _ = runner.embed(sentences)\n",
    "our_embedding = torch.cat([rep_cls_embedding, our_embedding, rep_sep_embedding], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d9a447",
   "metadata": {},
   "source": [
    "and run the transformer stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a639195e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05308860167860985, 0.5280542969703674, -0.24123799800872803, 0.0410262793302536, 0.42819151282310486, 0.18511953949928284, -0.7586131691932678, 0.2551998496055603, 0.2106182724237442, 0.17976105213165283, -0.5206780433654785, 0.2680550515651703, 0.21642565727233887, -0.6520727276802063, -0.5659886598587036, -0.5297321081161499]\n"
     ]
    }
   ],
   "source": [
    "model_result_3 = model(\n",
    "    inputs_embeds=our_embedding,\n",
    "#    attention_mask=tokenizer_result['attention_mask'],\n",
    "#    token_type_ids=tokenizer_result['token_type_ids'],\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "\n",
    "print(model_result_3['hidden_states'][0][0,1,:16].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfcbc6d",
   "metadata": {},
   "source": [
    "Finally let's print out the predictions in a readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb86838e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence    : I really enjoyed this movie\n",
      " Verdict (1): True\n",
      " Verdict (2): True\n",
      " Verdict (3): True\n",
      " Sanitized  : ['i really enjoys this movie']\n",
      "Sentence    : Worst two hours I have spent in my life\n",
      " Verdict (1): False\n",
      " Verdict (2): False\n",
      " Verdict (3): True\n",
      " Sanitized  : ['670 two hours i have spent in my life']\n",
      "Sentence    : My worries and fears about this movie were swept away within the first fifteen minutes\n",
      " Verdict (1): True\n",
      " Verdict (2): True\n",
      " Verdict (3): True\n",
      " Sanitized  : ['670rdon andrdon about this movie were swept away within the first 670 minutes']\n",
      "Sentence    : My worries and fears about this movie were fully confirmed within the first fifteen minutes\n",
      " Verdict (1): True\n",
      " Verdict (2): True\n",
      " Verdict (3): True\n",
      " Sanitized  : ['670rdon andrdon about this movie were fully 690 within the first 670 minutes']\n",
      "Sentence    : My worries and fears about this movie were rendered meaningless within the first fifteen minutes\n",
      " Verdict (1): False\n",
      " Verdict (2): False\n",
      " Verdict (3): True\n",
      " Sanitized  : ['670rdon andrdon about this movie wereanor 670 within the first 670 minutes']\n",
      "Sentence    : Movie was very good\n",
      " Verdict (1): True\n",
      " Verdict (2): True\n",
      " Verdict (3): True\n",
      " Sanitized  : ['##rdon was very good']\n",
      "Sentence    : It was enjoyable\n",
      " Verdict (1): True\n",
      " Verdict (2): True\n",
      " Verdict (3): False\n",
      " Sanitized  : ['it wasibar']\n",
      "Sentence    : Very boring unfortunately\n",
      " Verdict (1): False\n",
      " Verdict (2): False\n",
      " Verdict (3): False\n",
      " Sanitized  : ['670 670 unfortunately']\n"
     ]
    }
   ],
   "source": [
    "results = torch.argmax(model_result.logits, dim=1).tolist()\n",
    "results_2 = torch.argmax(model_result_2.logits, dim=1).tolist()\n",
    "results_3 = torch.argmax(model_result_3.logits, dim=1).tolist()\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print(\"Sentence    : {}\".format(sentences[i]))\n",
    "    print(\" Verdict (1): {}\".format(bool(results[i])))\n",
    "    print(\" Verdict (2): {}\".format(bool(results_2[i])))\n",
    "    print(\" Verdict (3): {}\".format(bool(results_3[i])))\n",
    "    print(\" Sanitized  : {}\".format(runner.sanitize([sentences[i]])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
