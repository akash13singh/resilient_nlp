{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faf07b01",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6afc56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from lstm import ExperimentRunner\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaed86d5",
   "metadata": {},
   "source": [
    "Load the tokenizer and the finetuned model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "036c9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"artemis13fowl/bert-base-uncased-imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7f7fa4",
   "metadata": {},
   "source": [
    "Load the trained tokenizer. It has only been trained for 2 epochs on 64k sentences, so it's not mind-blowing.\n",
    "\n",
    "For now you need to manually get it and put it in the root of the resilient_nlp repo, sorry. You can get the model here: https://resilient-nlp.s3.us-west-2.amazonaws.com/tok_bert_base_uncased_64k_sentences_2_epochs.pth and the vocab here: https://resilient-nlp.s3.us-west-2.amazonaws.com/tok_bert_base_uncased_64k_sentences_2_epochs_vocab.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59323f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = ExperimentRunner(device, model_name=\"bert-base-uncased\")\n",
    "runner.model.load(\"../tok_bert_base_uncased_64k_sentences_2_epochs.pth\", device)\n",
    "runner.char_tokenizer.load_vocab(\"../tok_bert_base_uncased_64k_sentences_2_epochs_vocab.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e617511",
   "metadata": {},
   "source": [
    "Some simple test sentences for sanity checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "740e6fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "  \"I really enjoyed this movie\",\n",
    "  \"Worst two hours I have spent in my life\",\n",
    "  \"My worries and fears about this movie were swept away within the first fifteen minutes\",\n",
    "  \"My worries and fears about this movie were fully confirmed within the first fifteen minutes\",\n",
    "  \"My worries and fears about this movie were rendered meaningless within the first fifteen minutes\",\n",
    "  \"Movie was very good\",\n",
    "  \"It was enjoyable\",\n",
    "  \"Very boring unfortunately\",\n",
    "  \"lt w@s 3njo yabl3\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8915c3c",
   "metadata": {},
   "source": [
    "First, use the regular bert-base-uncased tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da93dcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.010627184063196182, 0.5175461769104004, -0.2631071209907532, 0.04238038882613182, 0.40291687846183777, 0.19839873909950256, -0.7841276526451111, 0.3104897141456604, 0.25195378065109253, 0.1873702108860016, -0.5451260209083557, 0.27750512957572937, 0.20487289130687714, -0.6549283266067505, -0.551720142364502, -0.5405393242835999]\n"
     ]
    }
   ],
   "source": [
    "tokenizer_result = tokenizer(sentences, return_tensors='pt', padding=True)\n",
    "\n",
    "model_result = model(\n",
    "    input_ids=tokenizer_result['input_ids'],\n",
    "    attention_mask=tokenizer_result['attention_mask'],\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "\n",
    "print(model_result['hidden_states'][0][0,1,:16].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5eb3c0",
   "metadata": {},
   "source": [
    "Next, manually embed using BERT's embedding before passing to the model. This is to verify that the model behaves identically (i.e. it turns out that positional embeddings, etc. are added automatically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57a3ee65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.010627184063196182, 0.5175461769104004, -0.2631071209907532, 0.04238038882613182, 0.40291687846183777, 0.19839873909950256, -0.7841276526451111, 0.3104897141456604, 0.25195378065109253, 0.1873702108860016, -0.5451260209083557, 0.27750512957572937, 0.20487289130687714, -0.6549283266067505, -0.551720142364502, -0.5405393242835999]\n"
     ]
    }
   ],
   "source": [
    "cf_embedding = model.base_model.embeddings.word_embeddings\n",
    "cf_embedding_result = cf_embedding(tokenizer_result['input_ids'])\n",
    "\n",
    "model_result_2 = model(\n",
    "    inputs_embeds=cf_embedding_result,\n",
    "    attention_mask=tokenizer_result['attention_mask'],\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "\n",
    "print(model_result_2['hidden_states'][0][0,1,:16].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348fcd73",
   "metadata": {},
   "source": [
    "Now the fun part :). Let's use our trained embeddings. First though let's get the representations of the [CLS] and [SEP] tokens (not predicted by the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a98317d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token_id = tokenizer.vocab['[CLS]']\n",
    "sep_token_id = tokenizer.vocab['[SEP]']\n",
    "pad_token_id = tokenizer.vocab['[PAD]']\n",
    "cls_embedding = cf_embedding(torch.tensor([cls_token_id])).view(768)\n",
    "sep_embedding = cf_embedding(torch.tensor([sep_token_id])).view(768)\n",
    "pad_embedding = cf_embedding(torch.tensor([pad_token_id])).view(768)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fc9255",
   "metadata": {},
   "source": [
    "Now actually embed the input sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe0cc42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_embedding = runner.embed([s.lower() for s in sentences], start_token=cls_embedding, end_token=sep_embedding,\n",
    "    pad_token=pad_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d9a447",
   "metadata": {},
   "source": [
    "and run the transformer stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a639195e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.033297598361968994, 0.5032650232315063, -0.23978081345558167, 0.024146944284439087, 0.40309470891952515, 0.21676689386367798, -0.7777954936027527, 0.3020963668823242, 0.20578446984291077, 0.2136598527431488, -0.5167685151100159, 0.2698840796947479, 0.2521650791168213, -0.6605544686317444, -0.5743234157562256, -0.5343013405799866]\n"
     ]
    }
   ],
   "source": [
    "model_result_3 = model(\n",
    "    inputs_embeds=our_embedding['inputs_embeds'],\n",
    "    attention_mask=our_embedding['attention_mask'],\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "\n",
    "print(model_result_3['hidden_states'][0][0,1,:16].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfcbc6d",
   "metadata": {},
   "source": [
    "Finally let's print out the predictions in a readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb86838e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence                        : I really enjoyed this movie\n",
      " Vanilla tokenization/embedding : True\n",
      " MockingBERT                    : True\n",
      " Sanitized                      : i really enjoyed this movie\n",
      "\n",
      "Sentence                        : Worst two hours I have spent in my life\n",
      " Vanilla tokenization/embedding : False\n",
      " MockingBERT                    : False\n",
      " Sanitized                      : 1879 two hours i have spent in my life\n",
      "\n",
      "Sentence                        : My worries and fears about this movie were swept away within the first fifteen minutes\n",
      " Vanilla tokenization/embedding : True\n",
      " MockingBERT                    : True\n",
      " Sanitized                      : john worries and fears about this movie were swept away within the first fifteen minutes\n",
      "\n",
      "Sentence                        : My worries and fears about this movie were fully confirmed within the first fifteen minutes\n",
      " Vanilla tokenization/embedding : True\n",
      " MockingBERT                    : True\n",
      " Sanitized                      : john worries and fears about this movie were fully confirmed within the first fifteen minutes\n",
      "\n",
      "Sentence                        : My worries and fears about this movie were rendered meaningless within the first fifteen minutes\n",
      " Vanilla tokenization/embedding : False\n",
      " MockingBERT                    : True\n",
      " Sanitized                      : john worries and fears about this movie were [unused203] [unused162] within the first fifteen minutes\n",
      "\n",
      "Sentence                        : Movie was very good\n",
      " Vanilla tokenization/embedding : True\n",
      " MockingBERT                    : True\n",
      " Sanitized                      : [unused613] was very good\n",
      "\n",
      "Sentence                        : It was enjoyable\n",
      " Vanilla tokenization/embedding : True\n",
      " MockingBERT                    : True\n",
      " Sanitized                      : it was [unused395]\n",
      "\n",
      "Sentence                        : Very boring unfortunately\n",
      " Vanilla tokenization/embedding : False\n",
      " MockingBERT                    : False\n",
      " Sanitized                      : very boring unfortunately\n",
      "\n",
      "Sentence                        : lt w@s 3njo yabl3\n",
      " Vanilla tokenization/embedding : False\n",
      " MockingBERT                    : True\n",
      " Sanitized                      : it was [unused395]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = torch.argmax(model_result.logits, dim=1).tolist()\n",
    "results_2 = torch.argmax(model_result_2.logits, dim=1).tolist()\n",
    "results_3 = torch.argmax(model_result_3.logits, dim=1).tolist()\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    print(\"Sentence                        : {}\".format(sentences[i]))\n",
    "    print(\" Vanilla tokenization/embedding : {}\".format(bool(results[i])))\n",
    "    print(\" MockingBERT                    : {}\".format(bool(results_3[i])))\n",
    "    print(\" Sanitized                      : {}\".format(runner.sanitize([sentences[i]])[0]))\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
